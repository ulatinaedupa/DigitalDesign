{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detección por Webcam usando 'Harr Cascades' y Filtros de Detección de Borde (Canny)\n",
    "\n",
    "En este documento, utilizaremos OpenCV que es una librería de vision por computador para tomar datos de una webcam y:\n",
    "- Detectar Caras en \"Tiempo Real\" usando \"Haar Cascades\"\n",
    "- Detectar Bordes en \"Tiempo Real\" usando el filtro \"Canny\"\n",
    "\n",
    "Referencias:\n",
    "\n",
    "https://github.com/Itseez/opencv/blob/master/data/haarcascades/haarcascade_frontalface_default.xml\n",
    "https://github.com/Itseez/opencv/blob/master/data/haarcascades/haarcascade_eye.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 1: Cargar el Overlay\n",
    "\n",
    "Se importan el overlay basico de PYNQ al FPGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pynq import Overlay\n",
    "Overlay(\"base.bit\").download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 2: Inicializa los drivers de WebCam y HDMI\n",
    "\n",
    "Importamos los drivers de HDMI de la tarjeta y configuramos la salida HDMI para utilizar y desplegar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de video de 640*480 @ 60Hz\n",
    "from pynq.drivers import HDMI \n",
    "#from pynq.drivers.video import VMODE_640x480\n",
    "#hdmi_out = HDMI('out', video_mode=VMODE_640x480)\n",
    "from pynq.drivers.video import VMODE_1280x720\n",
    "hdmi_out = HDMI('out', video_mode=VMODE_1280x720)\n",
    "hdmi_out.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 3: Declaramos algunas constantes\n",
    "\n",
    "Constantes de salida para el monitor y la webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Salida del frame buffer del monitor\n",
    "frame_out_w = 1920\n",
    "frame_out_h = 1080\n",
    "# Configuración de la cámara de salida\n",
    "frame_in_w = 1280\n",
    "frame_in_h = 720"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 4:  Llamado de librerías\n",
    "\n",
    "Llamamos el overlay driver de trama como utilitario de las lecturas realizadas por la cámara.\n",
    "Importamos las librerías de OpenCV igualmente.  \n",
    "Hacemos llamado también a la librería numpy que nos ayudará a procesar arreglos y vectores.  \n",
    "Finalmente llamamos a la librería de botones que nos ayudará a poder interrumpir el código para saltar a las siguientes demostraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llamados a las librerias a utilizar\n",
    "from pynq.drivers import Frame\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pynq.board import Button"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 5:  Inicialización de cámara\n",
    "\n",
    "Abrimos una captura de video con las propiedades de las constantes declaradas anteriormente para procesar cada trama.  \n",
    "Cabe destacar que a una menor resolución el sistema se desempeñara de manera más rápida porque no tendrá muchos pixeles que procesar.  \n",
    "Si el driver de la cámara se ejecuta con éxito tendremos acceso a las tramas de la cámara."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capture device is open: True\n"
     ]
    }
   ],
   "source": [
    "videoIn = cv2.VideoCapture(0)\n",
    "videoIn.set(cv2.CAP_PROP_FRAME_WIDTH, frame_in_w);\n",
    "videoIn.set(cv2.CAP_PROP_FRAME_HEIGHT, frame_in_h);\n",
    "\n",
    "print(\"Capture device is open: \" + str(videoIn.isOpened()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 6: Uso de Haar Cascades\n",
    "\n",
    "Cargamos los modelos de vision para haarcascades que se encuentran dentro de nuestra tarjeta sd en esta.  \n",
    "Seguidamente pasamos los pixeles de la cámara a la pantalla y desplegamos en el monitor.  \n",
    "Aplicamos 'haar cascades' para detectar caras.  \n",
    "A las caras las escogemos como región de interes para luego detectar ojos.  \n",
    "Finalmente ponemos cajas para cada caso.  \n",
    "Desplegamos por HDMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga los modelos de Haar desde la tarjeta SD\n",
    "face_cascade = cv2.CascadeClassifier('./data/haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('./data/haarcascade_eye.xml')\n",
    "\n",
    "# Repite indefinidamente\n",
    "while (True):\n",
    "    ret, frame_webcam = videoIn.read() # captura una trama de video\n",
    "\n",
    "    # Despliega en la salida HDMI a todo color\n",
    "    if (ret):\n",
    "        frame_1080p = np.zeros((frame_out_h,frame_out_w,3)).astype(np.uint8)                  # la trama de 1080p\n",
    "        frame_1080p[0:frame_in_h,0:frame_in_w,:] = frame_webcam[0:frame_in_h,0:frame_in_w,:]  # match de la trama de webcam\n",
    "        hdmi_out.frame_raw(bytearray(frame_1080p.astype(np.int8).tobytes()))                  # desplegamos la salida\n",
    "    else:\n",
    "        raise RuntimeError(\"Failed to read from camera.\")                                     # error de lectura\n",
    "    np_frame = frame_webcam\n",
    "    \n",
    "    # cambiamos a modo de escala de grises y ajustamos los parámetros de deteccion en Haar\n",
    "    gray = cv2.cvtColor(np_frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    # buscamos los 'bounding boxes' o cajas donde se encuentran las caras\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(np_frame,(x,y),(x+w,y+h),(255,0,0),2) # dibujamos un rectángulo en la cara\n",
    "        roi_gray = gray[y:y+h, x:x+w]                       # tomamos una parte de la imagen a B&W\n",
    "        roi_color = np_frame[y:y+h, x:x+w]                  # tomamos una parte de la imagen a color\n",
    "\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)       # procedemos a detectar los ojos\n",
    "        for (ex,ey,ew,eh) in eyes:                          # a las imagenes de grises anteriores, detectamos ojos\n",
    "            cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2) # ubicamos un rectángulo\n",
    "    # Sacamos el resultado por HDMI\n",
    "    frame_1080p[0:frame_in_h,0:frame_in_w,:] = frame_webcam[0:frame_in_h,0:frame_in_w,:] # trama a 1080p a procesar\n",
    "    hdmi_out.frame_raw(bytearray(frame_1080p.astype(np.int8).tobytes()))  # convertimos la trama a bytes\n",
    "    if (Button(0).read()==1):                               # si detectamos el botón BTN0\n",
    "        break                                               # rompemos\n",
    "    del ret, frame_webcam, frame_1080p, np_frame, gray      # borramos variables temporales para evitar volcado de pila"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 7: Uso de Canny\n",
    "\n",
    "El filtro de Canny, nombre que obtiene por su creador, es uno de los filtros más conocidos para detección de bordes.  \n",
    "Los SDC o automoviles autónomos usan un filtro similar para detectar líneas de la calle.  \n",
    "Aquí solo la utilizaremos para detectar bordes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_1080p = np.zeros((frame_out_h,frame_out_w,3)).astype(np.uint8)\n",
    "while (True):\n",
    "    # Lee la siguiente imagen\n",
    "    ret, frame_webcam = videoIn.read()                  # captura una trama\n",
    "    if (ret):\n",
    "        frame_canny = cv2.Canny(frame_webcam,100,110)   # aplica el filtro canny\n",
    "        \n",
    "        for i in range(3):                              # copia a los tres canales \n",
    "            frame_1080p[0:frame_in_h,0:frame_in_w,i] = frame_canny[0:frame_in_h,0:frame_in_w]\n",
    "\n",
    "                                                        # Copia el frame buffer y lo despliega en el monitor\n",
    "        hdmi_out.frame_raw(bytearray(frame_1080p.astype(np.int8).tobytes()))\n",
    "    if (Button(0).read()==1):                           # Termina la ejecución si el botón 0 es presionado\n",
    "        break\n",
    "    del ret, frame_webcam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 8: Liberamos los recursos de HDMI y cámara\n",
    "\n",
    "Finalmente liberamos el HDMI y la cámara de la memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoIn.release()       # retiramos el driver de la webcam\n",
    "hdmi_out.stop()         # retiramos el driver de hdmi\n",
    "del hdmi_out, videoIn   # borramos las variables temporales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
